{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b758e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from pathlib2 import Path\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8af2b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "okenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batched_data = []\n",
    "    batched_targets = []\n",
    "    paths = []\n",
    "    \n",
    "    for text, targets, path in batch:\n",
    "        paths.append(path)\n",
    "        \n",
    "        tensor_targets = torch.LongTensor(targets)\n",
    "        batched_targets.append(tensor_targets)\n",
    "        \n",
    "        bert_input = tokenizer.batch_encode_plus(text, pad_to_max_length=True, return_tensors='pt')\n",
    "        batched_data.append(bert_input)\n",
    "        \n",
    "    return batched_data, batched_targets, paths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c78a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRCDdataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        super().__init__()\n",
    "        self.files = list(Path(data_path).glob('*.txt'))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.files[index]\n",
    "        \n",
    "        return self.read_DRCD_file(path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def read_DRCD_file(self, path):\n",
    "        seperator = '=========='\n",
    "        with Path(path).open('r', encoding='utf-8') as f:\n",
    "            raw_text = f.read()\n",
    "        paragraphs = [p for p in raw_text.strip().split(seperator) if len(p)>2]\n",
    "        \n",
    "        targets = []\n",
    "        text = []\n",
    "        for paragraph in paragraphs:\n",
    "            sentences = [s for s in paragraph.split('\\n') if len(s.split()) > 0]\n",
    "            sentences_targets = [0 for s in sentences[:-1]]\n",
    "            sentences_targets.append(1)\n",
    "            targets.extend(sentences_targets)\n",
    "        \n",
    "        \n",
    "            for sentence in sentences:\n",
    "                text.append(sentence)\n",
    "        \n",
    "        return text, targets, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bbe37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_dim, hidden_layer, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = BertConfig.from_pretrained('bert-base-chinese', output_hidden_states=True)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese', config=self.config)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(768, hidden_dim, hidden_layer, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_dim * 2,2)\n",
    "        \n",
    "    def pad_document(self, d, max_document_length):\n",
    "        d_length = d.size()[0]\n",
    "        v = d.unsqueeze(0).unsqueeze(0)\n",
    "        padded = F.pad(v, (0,0,0, max_document_length - d_length ))  # (1, 1, max_length, 768)\n",
    "        shape = padded.size()\n",
    "        return padded.view(shape[2], 1, shape[3])  # (max_length, 1, 768)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        batched_cls_lhs = []\n",
    "        doc_len = []\n",
    "        for x in batch:\n",
    "            _, cls_lhs, _ = self.bert(x['input_ids'], x['attention_mask'], return_dict=False)\n",
    "            doc_len.append(cls_lhs.shape[0])\n",
    "            batched_cls_lhs.append(cls_lhs)\n",
    "        max_doc_len = max(doc_len)\n",
    "        padded_doc = [self.pad_document(d,max_doc_len) for d in batched_cls_lhs]\n",
    "        docs_tensor = torch.cat(padded_doc, 1)\n",
    "        \n",
    "        x, _ = self.lstm(docs_tensor)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e703b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DRCDdataset(data_path=r'C:\\Users\\vince_wang\\research\\evaluate\\8.24\\DRCD\\train')\n",
    "train_dl = DataLoader(train_dataset, batch_size=3, collate_fn=collate_fn, shuffle=True)\n",
    "val_dl = DataLoader(train_dataset, batch_size=5, collate_fn=collate_fn, shuffle=True)\n",
    "model = Model(300,1,1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d6d7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4de53e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8476\\2969787648.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mstored\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstored\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "from lstm import DRCDdataset\n",
    "\n",
    "train_dataset = DRCDdataset(data_path=r'C:\\Users\\vince_wang\\research\\evaluate\\8.24\\DRCD\\train')\n",
    "train_dl = DataLoader(train_dataset, batch_size=3, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "stored = []\n",
    "for i, (data, targets, path) in enumerate(val_dl):\n",
    "    stored.append(data)\n",
    "    break\n",
    "tensor = torch.cat(stored, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91641a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensored_targets = torch.zeros(6).long()\n",
    "tensored_targets[torch.LongTensor([1,3])] = 1\n",
    "tensored_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbcb8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 300])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(10,5,300)\n",
    "b = t[0:5,1,:]\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6675d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20356\\2820624454.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\vince_wang\\Anaconda3\\envs\\bert\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "torch.zeros(3,4).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247cbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 5], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2],[2,3],[4,5]])\n",
    "a = a.numpy()\n",
    "(a[0:3])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ee8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2])\n",
    "b = [a,a,a]\n",
    "torch.cat(b,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31fee049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lstm import DRCDdataset, Model, collate_fn\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from pathlib import Path\n",
    "from torch.autograd import Variable\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "utils.read_config_file('config.json')\n",
    "utils.config.update({'cuda':False})\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "dataset_path = Path(utils.config['DRCDdataset'])\n",
    "train_dataset = DRCDdataset(dataset_path / 'test')\n",
    "train_dl = DataLoader(train_dataset, batch_size=1, collate_fn=collate_fn, shuffle=True)\n",
    "model = Model(300, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15fe8e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2 [00:35<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7525798976421356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_loss = float(0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "with tqdm(desc='Training', total=len(train_dl)) as pbar:\n",
    "    for data,target,_ in train_dl:\n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        target_var = Variable(torch.cat(target, 0), requires_grad=False)\n",
    "        loss = model.criterion(output, target_var)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "total_loss = total_loss / len(train_dl)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e667707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "def pad_document(d, max_document_length):\n",
    "    d_length = d.size()[0]\n",
    "    v = d.unsqueeze(0).unsqueeze(0)\n",
    "    # 進行 padding \n",
    "    padded = F.pad(v, (0,0,0, max_document_length - d_length ))  # (1, 1, max_length, 768)\n",
    "    shape = padded.size()\n",
    "    return padded.view(shape[2], 1, shape[3])  # (max_length, 1, 768)\n",
    "\n",
    "def forward(batch):\n",
    "    config = BertConfig.from_pretrained('bert-base-chinese', output_hidden_states=True)\n",
    "    bert = BertModel.from_pretrained('bert-base-chinese', config=config)\n",
    "    hidden_dim = 300\n",
    "    hidden_layer = 1\n",
    "    batch_size = 1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    lstm = nn.LSTM(768, hidden_dim, hidden_layer, bidirectional=True)\n",
    "    linear = nn.Linear(hidden_dim * 2,2)\n",
    "        \n",
    "    batched_cls_lhs = []\n",
    "    doc_len = []\n",
    "    for x in batch:\n",
    "        _, cls_lhs, _ = bert(x['input_ids'], x['attention_mask'], return_dict=False)\n",
    "        print(cls_lhs.shape)\n",
    "        doc_len.append(cls_lhs.shape[0])\n",
    "        batched_cls_lhs.append(cls_lhs)\n",
    "    max_doc_len = max(doc_len)\n",
    "    padded_doc = [pad_document(d,max_doc_len) for d in batched_cls_lhs]\n",
    "    docs_tensor = torch.cat(padded_doc, 1)\n",
    "    \n",
    "    x, _ = lstm(docs_tensor)\n",
    "    \n",
    "    doc_outputs = []\n",
    "    for i, size in enumerate(doc_len):\n",
    "        doc_outputs.append(x[0:size, i, :])\n",
    "    x = torch.cat(doc_outputs, 0)\n",
    "    \n",
    "    x = linear(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "for data,target,_ in train_dl:\n",
    "    forward(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "15e60dedaf07597f2d6c73a00eb0e901b38f2847b365819b218b0f1d1ef4cd59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
